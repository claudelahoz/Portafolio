{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import DataFrame\n",
    "from pyspark.sql.functions import col, count, when\n",
    "from typing import Tuple\n",
    "\n",
    "def split_train_test_three_categories(\n",
    "    df: DataFrame,\n",
    "    target_column: str,\n",
    "    train_ratio: float = 0.8,\n",
    "    seed: int = 42\n",
    ") -> Tuple[DataFrame, DataFrame]:\n",
    "    \"\"\"\n",
    "    Divide un DataFrame de Spark en conjuntos de entrenamiento y prueba, \n",
    "    manteniendo la proporción de las tres categorías del target.\n",
    "    \n",
    "    Args:\n",
    "        df (DataFrame): DataFrame de Spark preprocesado.\n",
    "        target_column (str): Nombre de la columna objetivo con tres categorías.\n",
    "        train_ratio (float): Proporción de datos para el conjunto de entrenamiento. Por defecto es 0.8 (80%).\n",
    "        seed (int): Semilla para reproducibilidad. Por defecto es 42.\n",
    "    \n",
    "    Returns:\n",
    "        Tuple[DataFrame, DataFrame]: DataFrames de entrenamiento y prueba.\n",
    "    \"\"\"\n",
    "    if train_ratio <= 0 or train_ratio >= 1:\n",
    "        raise ValueError(\"train_ratio debe estar entre 0 y 1\")\n",
    "    \n",
    "    # Verificar que hay exactamente tres categorías\n",
    "    categories = df.select(target_column).distinct().count()\n",
    "    if categories != 3:\n",
    "        raise ValueError(f\"El target debe tener exactamente 3 categorías, pero tiene {categories}\")\n",
    "    \n",
    "    # Contar el número de registros por categoría\n",
    "    class_counts = df.groupBy(target_column).count().collect()\n",
    "    \n",
    "    # Calcular la fracción para cada categoría para mantener la estratificación\n",
    "    fractions = {row[target_column]: train_ratio for row in class_counts}\n",
    "    \n",
    "    # Muestreo estratificado\n",
    "    train = df.sampleBy(target_column, fractions, seed)\n",
    "    test = df.subtract(train)\n",
    "    \n",
    "    # Función para calcular y mostrar la distribución de clases\n",
    "    def print_class_distribution(data: DataFrame, name: str):\n",
    "        total = data.count()\n",
    "        distribution = data.groupBy(target_column).agg(\n",
    "            count(\"*\").alias(\"count\"),\n",
    "            (count(\"*\") / total * 100).alias(\"percentage\")\n",
    "        ).orderBy(target_column)\n",
    "        print(f\"\\nDistribución de clases en {name} (Total: {total}):\")\n",
    "        distribution.show()\n",
    "\n",
    "    # Mostrar la distribución de clases en los conjuntos completo, de entrenamiento y de prueba\n",
    "    print_class_distribution(df, \"el conjunto completo\")\n",
    "    print_class_distribution(train, \"el conjunto de entrenamiento\")\n",
    "    print_class_distribution(test, \"el conjunto de prueba\")\n",
    "    \n",
    "    # Verificar que la división se hizo correctamente\n",
    "    total_count = df.count()\n",
    "    train_count = train.count()\n",
    "    test_count = test.count()\n",
    "    \n",
    "    print(f\"\\nResumen de la división:\")\n",
    "    print(f\"Total de registros: {total_count}\")\n",
    "    print(f\"Registros de entrenamiento: {train_count} ({train_count/total_count:.2%})\")\n",
    "    print(f\"Registros de prueba: {test_count} ({test_count/total_count:.2%})\")\n",
    "    \n",
    "    if abs((train_count / total_count) - train_ratio) > 0.01:\n",
    "        print(\"Advertencia: La proporción de división real difiere de la solicitada en más del 1%\")\n",
    "    \n",
    "    return train, test\n",
    "\n",
    "# Ejemplo de uso:\n",
    "# preprocessed_df = ... # Tu DataFrame de Spark preprocesado\n",
    "# target_column = \"clasificacion\"\n",
    "# train_df, test_df = split_train_test_three_categories(preprocessed_df, target_column, train_ratio=0.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import DataFrame\n",
    "from pyspark.sql.functions import col, count, lit, rand, expr\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.linalg import Vectors, VectorUDT\n",
    "from pyspark.sql.types import StructType, StructField, DoubleType\n",
    "import pyspark.sql.functions as F\n",
    "from typing import List\n",
    "\n",
    "def optimal_balance_training(\n",
    "    df: DataFrame,\n",
    "    target_column: str,\n",
    "    feature_column: str,\n",
    "    balance_ratio: float = 0.8,\n",
    "    k_neighbors: int = 5,\n",
    "    seed: int = 42\n",
    ") -> DataFrame:\n",
    "    \"\"\"\n",
    "    Balancea la muestra de entrenamiento usando una combinación de técnicas para minimizar el sobreajuste.\n",
    "    \n",
    "    Args:\n",
    "        df (DataFrame): DataFrame de Spark con la muestra de entrenamiento.\n",
    "        target_column (str): Nombre de la columna objetivo con tres categorías.\n",
    "        feature_column (str): Nombre de la columna que contiene el vector de características.\n",
    "        balance_ratio (float): Proporción objetivo para el balanceo (0.8 significa que las clases minoritarias\n",
    "                               se sobremuestrearán hasta el 80% de la clase mayoritaria).\n",
    "        k_neighbors (int): Número de vecinos a considerar para la generación de muestras sintéticas.\n",
    "        seed (int): Semilla para reproducibilidad.\n",
    "    \n",
    "    Returns:\n",
    "        DataFrame: DataFrame de Spark con la muestra balanceada.\n",
    "    \"\"\"\n",
    "    # Contar las instancias de cada clase\n",
    "    class_counts = df.groupBy(target_column).count().collect()\n",
    "    total_counts = {row[target_column]: row['count'] for row in class_counts}\n",
    "    \n",
    "    # Encontrar la clase mayoritaria y su conteo\n",
    "    majority_class = max(total_counts, key=total_counts.get)\n",
    "    majority_count = total_counts[majority_class]\n",
    "    \n",
    "    # Calcular el conteo objetivo para las clases minoritarias\n",
    "    target_count = int(majority_count * balance_ratio)\n",
    "    \n",
    "    # Función para generar instancias sintéticas usando SMOTE\n",
    "    def generate_smote_samples(df: DataFrame, target_value: any, n_samples: int, k: int) -> DataFrame:\n",
    "        # Seleccionar instancias de la clase minoritaria\n",
    "        minority_samples = df.filter(col(target_column) == target_value)\n",
    "        \n",
    "        # Función para encontrar los k vecinos más cercanos\n",
    "        def find_k_neighbors(vector: List[float], k: int) -> List[List[float]]:\n",
    "            return minority_samples.rdd.map(\n",
    "                lambda row: (sum((a - b) ** 2 for a, b in zip(vector, row[feature_column])), row)\n",
    "            ).sortByKey().map(lambda x: x[1][feature_column]).take(k + 1)[1:]\n",
    "        \n",
    "        # Función para generar una nueva instancia\n",
    "        def generate_instance(row):\n",
    "            features = row[feature_column]\n",
    "            neighbors = find_k_neighbors(features, k)\n",
    "            new_features = []\n",
    "            for i in range(len(features)):\n",
    "                diff = neighbors[rand.randint(0, k-1)][i] - features[i]\n",
    "                new_features.append(features[i] + rand.random() * diff)\n",
    "            return (Vectors.dense(new_features), row[target_column])\n",
    "        \n",
    "        # Generar nuevas instancias\n",
    "        synthetic_samples = minority_samples.rdd.flatMap(\n",
    "            lambda row: [generate_instance(row) for _ in range(n_samples // minority_samples.count())]\n",
    "        ).toDF([feature_column, target_column])\n",
    "        \n",
    "        return synthetic_samples\n",
    "    \n",
    "    # Balancear cada clase\n",
    "    balanced_dfs = []\n",
    "    for class_value, count in total_counts.items():\n",
    "        if count > target_count:\n",
    "            # Submuestreo para la clase mayoritaria\n",
    "            balanced_dfs.append(df.filter(col(target_column) == class_value).sample(fraction=target_count/count, seed=seed))\n",
    "        elif count < target_count:\n",
    "            # Sobremuestreo para las clases minoritarias\n",
    "            balanced_dfs.append(df.filter(col(target_column) == class_value))\n",
    "            synthetic_samples = generate_smote_samples(\n",
    "                df, class_value, target_count - count, k_neighbors\n",
    "            )\n",
    "            balanced_dfs.append(synthetic_samples)\n",
    "        else:\n",
    "            # Mantener sin cambios si ya está en el conteo objetivo\n",
    "            balanced_dfs.append(df.filter(col(target_column) == class_value))\n",
    "    \n",
    "    # Unir todos los DataFrames\n",
    "    balanced_df = balanced_dfs[0].unionAll(*balanced_dfs[1:])\n",
    "    \n",
    "    # Añadir ruido gaussiano a las características para aumentar la variabilidad\n",
    "    @F.udf(returnType=VectorUDT())\n",
    "    def add_gaussian_noise(vector):\n",
    "        return Vectors.dense([v + rand.gauss(0, 0.01) for v in vector])\n",
    "    \n",
    "    balanced_df = balanced_df.withColumn(feature_column, add_gaussian_noise(col(feature_column)))\n",
    "    \n",
    "    # Mostrar la distribución de clases después del balanceo\n",
    "    print(\"\\nDistribución de clases después del balanceo:\")\n",
    "    balanced_df.groupBy(target_column).count().orderBy(target_column).show()\n",
    "    \n",
    "    return balanced_df\n",
    "\n",
    "# Ejemplo de uso:\n",
    "# train_df = ... # Tu DataFrame de Spark con la muestra de entrenamiento\n",
    "# target_column = \"clasificacion\"\n",
    "# feature_column = \"features\"\n",
    "# balanced_train_df = optimal_balance_training(train_df, target_column, feature_column)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
